{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Transform, Load (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run in terminal\n",
    "# wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0221EN-SkillsNetwork/labs/module%206/Lab%20-%20Extract%20Transform%20Load/data/source.zip\n",
    "# unzip source.zip\n",
    "\n",
    "import glob \n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as ET \n",
    "from datetime import datetime \n",
    "  \n",
    "log_file = \"log_file.txt\" \n",
    "target_file = \"transformed_data.csv\" \n",
    "  \n",
    "def extract_from_csv(file_to_process): \n",
    "    dataframe = pd.read_csv(file_to_process) \n",
    "    return dataframe \n",
    "  \n",
    "def extract_from_json(file_to_process): \n",
    "    dataframe = pd.read_json(file_to_process, lines=True) \n",
    "    return dataframe \n",
    "  \n",
    "def extract_from_xml(file_to_process): \n",
    "    dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"]) \n",
    "    tree = ET.parse(file_to_process) \n",
    "    root = tree.getroot() \n",
    "    for person in root: \n",
    "        name = person.find(\"name\").text \n",
    "        height = float(person.find(\"height\").text) \n",
    "        weight = float(person.find(\"weight\").text) \n",
    "        dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name,\"height\":height, \"weight\":weight}])], ignore_index=True) \n",
    "    return dataframe \n",
    "  \n",
    "def extract(): \n",
    "    extracted_data = pd.DataFrame(columns=['name','height','weight'])  \n",
    "# create an empty data frame to hold extracted data \n",
    "     \n",
    "    # process all csv files \n",
    "    for csvfile in glob.glob(\"*.csv\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index=True) \n",
    "         \n",
    "    # process all json files \n",
    "    for jsonfile in glob.glob(\"*.json\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_json(jsonfile))], ignore_index=True) \n",
    "     \n",
    "    # process all xml files \n",
    "    for xmlfile in glob.glob(\"*.xml\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_xml(xmlfile))], ignore_index=True) \n",
    "         \n",
    "    return extracted_data \n",
    "  \n",
    "def transform(data): \n",
    "    # Convert inches to meters and round off to two decimals \n",
    "    # 1 inch is 0.0254 meters \n",
    "    data['height'] = round(data.height * 0.0254,2) \n",
    "     \n",
    "    # Convert pounds to kilograms and round off to two decimals \n",
    "    # 1 pound is 0.45359237 kilograms \n",
    "    data['weight'] = round(data.weight * 0.45359237,2) \n",
    "     \n",
    "    return data \n",
    "  \n",
    "def load_data(target_file, transformed_data): \n",
    "    transformed_data.to_csv(target_file) \n",
    "  \n",
    "def log_progress(message): \n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second \n",
    "    now = datetime.now() # get current timestamp \n",
    "    timestamp = now.strftime(timestamp_format) \n",
    "    with open(log_file,\"a\") as f: \n",
    "        f.write(timestamp + ',' + message + '\\n') \n",
    "  \n",
    "# Log the initialization of the ETL process \n",
    "log_progress(\"ETL Job Started\") \n",
    "  \n",
    "# Log the beginning of the Extraction process \n",
    "log_progress(\"Extract phase Started\") \n",
    "extracted_data = extract() \n",
    "  \n",
    "# Log the completion of the Extraction process \n",
    "log_progress(\"Extract phase Ended\") \n",
    "  \n",
    "# Log the beginning of the Transformation process \n",
    "log_progress(\"Transform phase Started\") \n",
    "transformed_data = transform(extracted_data) \n",
    "print(\"Transformed Data\") \n",
    "print(transformed_data) \n",
    "  \n",
    "# Log the completion of the Transformation process \n",
    "log_progress(\"Transform phase Ended\") \n",
    "  \n",
    "# Log the beginning of the Loading process \n",
    "log_progress(\"Load phase Started\") \n",
    "load_data(target_file,transformed_data) \n",
    "  \n",
    "# Log the completion of the Loading process \n",
    "log_progress(\"Load phase Ended\") \n",
    "  \n",
    "# Log the completion of the ETL process \n",
    "log_progress(\"ETL Job Ended\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping and Extracting Data using APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://web.archive.org/web/20230902185655/https://en.everybodywiki.com/100_Most_Highly-Ranked_Films'\n",
    "db_name = 'Movies.db'\n",
    "table_name = 'Top_50'\n",
    "csv_path = '/home/project/top_50_films.csv'\n",
    "df = pd.DataFrame(columns=[\"Average Rank\",\"Film\",\"Year\"])\n",
    "count = 0\n",
    "\n",
    "html_page = requests.get(url).text\n",
    "data = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "tables = data.find_all('tbody')\n",
    "rows = tables[0].find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    if count<50:\n",
    "        col = row.find_all('td')\n",
    "        if len(col)!=0:\n",
    "            data_dict = {\"Average Rank\": col[0].contents[0],\n",
    "                         \"Film\": col[1].contents[0],\n",
    "                         \"Year\": col[2].contents[0]}\n",
    "            df1 = pd.DataFrame(data_dict, index=[0])\n",
    "            df = pd.concat([df,df1], ignore_index=True)\n",
    "            count+=1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "df.to_csv(csv_path)\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Databases using Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the file needed\n",
    "# wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-PY0221EN-Coursera/labs/v2/INSTRUCTOR.csv\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the SQLite3 service\n",
    "conn = sqlite3.connect('STAFF.db')\n",
    "\n",
    "# Define table parameters\n",
    "table_name = 'INSTRUCTOR'\n",
    "attribute_list = ['ID', 'FNAME', 'LNAME', 'CITY', 'CCODE']\n",
    "\n",
    "# Read the CSV data\n",
    "file_path = '/home/project/INSTRUCTOR.csv'\n",
    "df = pd.read_csv(file_path, names = attribute_list)\n",
    "\n",
    "# Load the CSV to the database\n",
    "df.to_sql(table_name, conn, if_exists = 'replace', index = False)\n",
    "print('Table is ready')\n",
    "\n",
    "# Query 1: Display all rows of the table\n",
    "query_statement = f\"SELECT * FROM {table_name}\"\n",
    "query_output = pd.read_sql(query_statement, conn)\n",
    "print(query_statement)\n",
    "print(query_output)\n",
    "\n",
    "# Query 2: Display only the FNAME column for the full table.\n",
    "query_statement = f\"SELECT FNAME FROM {table_name}\"\n",
    "query_output = pd.read_sql(query_statement, conn)\n",
    "print(query_statement)\n",
    "print(query_output)\n",
    "\n",
    "# Query 3: Display the count of the total number of rows.\n",
    "query_statement = f\"SELECT COUNT(*) FROM {table_name}\"\n",
    "query_output = pd.read_sql(query_statement, conn)\n",
    "print(query_statement)\n",
    "print(query_output)\n",
    "\n",
    "# Define data to be appended\n",
    "data_dict = {'ID' : [100],\n",
    "            'FNAME' : ['John'],\n",
    "            'LNAME' : ['Doe'],\n",
    "            'CITY' : ['Paris'],\n",
    "            'CCODE' : ['FR']}\n",
    "data_append = pd.DataFrame(data_dict)\n",
    "\n",
    "# Append data to the table\n",
    "data_append.to_sql(table_name, conn, if_exists = 'append', index = False)\n",
    "print('Data appended successfully')\n",
    "\n",
    "# Query 4: Display the count of the total number of rows.\n",
    "query_statement = f\"SELECT COUNT(*) FROM {table_name}\"\n",
    "query_output = pd.read_sql(query_statement, conn)\n",
    "print(query_statement)\n",
    "print(query_output)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Transform and Load GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * from Countries_by_GDP WHERE GDP_USD_billions >= 100\n",
      "          Country  GDP_USD_billions\n",
      "0   United States          26854.60\n",
      "1           China          19373.59\n",
      "2           Japan           4409.74\n",
      "3         Germany           4308.85\n",
      "4           India           3736.88\n",
      "..            ...               ...\n",
      "64          Kenya            118.13\n",
      "65         Angola            117.88\n",
      "66           Oman            104.90\n",
      "67      Guatemala            102.31\n",
      "68       Bulgaria            100.64\n",
      "\n",
      "[69 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Code for ETL operations on Country-GDP data\n",
    "\n",
    "# Importing the required libraries\n",
    "\n",
    "def extract(url, table_attribs):\n",
    "    ''' This function extracts the required\n",
    "    information from the website and saves it to a dataframe. The\n",
    "    function returns the dataframe for further processing. '''\n",
    "    \n",
    "    page = requests.get(url).text\n",
    "    data = BeautifulSoup(page, 'html.parser')\n",
    "    df = pd.DataFrame(columns=table_attribs)\n",
    "    tables = data.find_all('tbody')\n",
    "    rows = tables[2].find_all('tr')\n",
    "    for row in rows:\n",
    "        col = row.find_all('td')\n",
    "        if (len(col)==0):\n",
    "            continue\n",
    "        if (col[0].find('a')==None) | ('—' in col[2]):\n",
    "            continue\n",
    "        data_dict = {\"Country\": col[0].find('a').text,\n",
    "                    #  \"Country\": col[0].a.contents[0],\n",
    "                     \"GDP_USD_millions\": col[2].contents[0]}\n",
    "                     #  \"GDP_USD_millions\": int(col[2].contents[0].replace(\",\", \"\"))}\n",
    "        df1 = pd.DataFrame(data_dict, index=[0])\n",
    "        df = pd.concat([df,df1], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform(df):\n",
    "    ''' This function converts the GDP information from Currency\n",
    "    format to float value, transforms the information of GDP from\n",
    "    USD (Millions) to USD (Billions) rounding to 2 decimal places.\n",
    "    The function returns the transformed dataframe.'''\n",
    "    \n",
    "    GDP_list = df['GDP_USD_millions'].tolist()\n",
    "    GDP_list = list(map(lambda x: np.round(float(x.replace(',', ''))/1000, 2), GDP_list))\n",
    "    df[\"GDP_USD_millions\"] = GDP_list\n",
    "    df=df.rename(columns = {\"GDP_USD_millions\":\"GDP_USD_billions\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_to_csv(df, csv_path):\n",
    "    ''' This function saves the final dataframe as a `CSV` file \n",
    "    in the provided path. Function returns nothing.'''\n",
    "\n",
    "    df.to_csv(csv_path)\n",
    "\n",
    "def load_to_db(df, sql_connection, table_name):\n",
    "    ''' This function saves the final dataframe as a database table\n",
    "    with the provided name. Function returns nothing.'''\n",
    "\n",
    "    df.to_sql(table_name, sql_connection, if_exists='replace', index=False)\n",
    "\n",
    "def run_query(query_statement, sql_connection):\n",
    "    ''' This function runs the stated query on the database table and\n",
    "    prints the output on the terminal. Function returns nothing. '''\n",
    "\n",
    "    print(query_statement)\n",
    "    query_output = pd.read_sql(query_statement, sql_connection)\n",
    "    print(query_output)\n",
    "\n",
    "def log_progress(message):\n",
    "    ''' This function logs the mentioned message at a given stage of the code execution to a log file. Function returns nothing'''\n",
    "\n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second \n",
    "    now = datetime.now() # get current timestamp \n",
    "    timestamp = now.strftime(timestamp_format) \n",
    "    with open(\"./etl_project_log.txt\",\"a\") as f: \n",
    "        f.write(timestamp + ' : ' + message + '\\n')\n",
    "\n",
    "''' Here, you define the required entities and call the relevant \n",
    "functions in the correct order to complete the project. Note that this\n",
    "portion is not inside any function.'''\n",
    "\n",
    "\n",
    "url = 'https://web.archive.org/web/20230902185326/https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29'\n",
    "table_attribs = ['Country', 'GDP_USD_millions']\n",
    "db_name = 'World_Economies.db'\n",
    "table_name = 'Countries_by_GDP'\n",
    "csv_path = 'Countries_by_GDP.csv'\n",
    "\n",
    "\n",
    "log_progress('Preliminaries complete. Initiating ETL process')\n",
    "\n",
    "df = extract(url, table_attribs)\n",
    "\n",
    "log_progress('Data extraction complete. Initiating Transformation process')\n",
    "\n",
    "df = transform(df)\n",
    "\n",
    "log_progress('Data transformation complete. Initiating loading process')\n",
    "\n",
    "load_to_csv(df, csv_path)\n",
    "\n",
    "log_progress('Data saved to CSV file')\n",
    "\n",
    "sql_connection = sqlite3.connect('World_Economies.db')\n",
    "\n",
    "log_progress('SQL Connection initiated.')\n",
    "\n",
    "load_to_db(df, sql_connection, table_name)\n",
    "\n",
    "log_progress('Data loaded to Database as table. Running the query')\n",
    "\n",
    "query_statement = f\"SELECT * from {table_name} WHERE GDP_USD_billions >= 100\"\n",
    "run_query(query_statement, sql_connection)\n",
    "\n",
    "log_progress('Process Complete.')\n",
    "\n",
    "sql_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Acquiring and processing information on world's largest banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Name  MC_USD_Billion\n",
      "0                           JPMorgan Chase          432.92\n",
      "1                          Bank of America          231.52\n",
      "2  Industrial and Commercial Bank of China          194.56\n",
      "3               Agricultural Bank of China          160.68\n",
      "4                                HDFC Bank          157.91\n",
      "5                              Wells Fargo          155.87\n",
      "6                        HSBC Holdings PLC          148.90\n",
      "7                           Morgan Stanley          140.83\n",
      "8                  China Construction Bank          139.82\n",
      "9                            Bank of China          136.81\n",
      "                                      Name  MC_USD_Billion  MC_EUR_Billion  \\\n",
      "0                           JPMorgan Chase          432.92          402.62   \n",
      "1                          Bank of America          231.52          215.31   \n",
      "2  Industrial and Commercial Bank of China          194.56          180.94   \n",
      "3               Agricultural Bank of China          160.68          149.43   \n",
      "4                                HDFC Bank          157.91          146.86   \n",
      "5                              Wells Fargo          155.87          144.96   \n",
      "6                        HSBC Holdings PLC          148.90          138.48   \n",
      "7                           Morgan Stanley          140.83          130.97   \n",
      "8                  China Construction Bank          139.82          130.03   \n",
      "9                            Bank of China          136.81          127.23   \n",
      "\n",
      "   MC_GBP_Billion  MC_INR_Billion  \n",
      "0          346.34        35910.71  \n",
      "1          185.22        19204.58  \n",
      "2          155.65        16138.75  \n",
      "3          128.54        13328.41  \n",
      "4          126.33        13098.63  \n",
      "5          124.70        12929.42  \n",
      "6          119.12        12351.26  \n",
      "7          112.66        11681.85  \n",
      "8          111.86        11598.07  \n",
      "9          109.45        11348.39  \n",
      "SELECT * FROM Largest_banks\n",
      "                                      Name  MC_USD_Billion  MC_EUR_Billion  \\\n",
      "0                           JPMorgan Chase          432.92          402.62   \n",
      "1                          Bank of America          231.52          215.31   \n",
      "2  Industrial and Commercial Bank of China          194.56          180.94   \n",
      "3               Agricultural Bank of China          160.68          149.43   \n",
      "4                                HDFC Bank          157.91          146.86   \n",
      "5                              Wells Fargo          155.87          144.96   \n",
      "6                        HSBC Holdings PLC          148.90          138.48   \n",
      "7                           Morgan Stanley          140.83          130.97   \n",
      "8                  China Construction Bank          139.82          130.03   \n",
      "9                            Bank of China          136.81          127.23   \n",
      "\n",
      "   MC_GBP_Billion  MC_INR_Billion  \n",
      "0          346.34        35910.71  \n",
      "1          185.22        19204.58  \n",
      "2          155.65        16138.75  \n",
      "3          128.54        13328.41  \n",
      "4          126.33        13098.63  \n",
      "5          124.70        12929.42  \n",
      "6          119.12        12351.26  \n",
      "7          112.66        11681.85  \n",
      "8          111.86        11598.07  \n",
      "9          109.45        11348.39  \n",
      "SELECT AVG(MC_GBP_Billion) FROM Largest_banks\n",
      "   AVG(MC_GBP_Billion)\n",
      "0              151.987\n",
      "SELECT Name from Largest_banks LIMIT 5\n",
      "                                      Name\n",
      "0                           JPMorgan Chase\n",
      "1                          Bank of America\n",
      "2  Industrial and Commercial Bank of China\n",
      "3               Agricultural Bank of China\n",
      "4                                HDFC Bank\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Get exchange_rate.csv file by running the following command in the terminal:\n",
    "# wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-PY0221EN-Coursera/labs/v2/exchange_rate.csv\n",
    "\n",
    "\n",
    "# Code for ETL operations on Country-GDP data\n",
    "\n",
    "# Importing the required libraries\n",
    "\n",
    "def log_progress(message):\n",
    "    ''' This function logs the mentioned message of a given stage of the\n",
    "    code execution to a log file. Function returns nothing'''\n",
    "    \n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second \n",
    "    now = datetime.now() # get current timestamp \n",
    "    timestamp = now.strftime(timestamp_format) \n",
    "    with open(\"./code_log.txt\",\"a\") as f: \n",
    "        f.write(timestamp + ' : ' + message + '\\n')\n",
    "\n",
    "def extract(url, table_attribs):\n",
    "    ''' This function aims to extract the required\n",
    "    information from the website and save it to a data frame. The\n",
    "    function returns the data frame for further processing. '''\n",
    "    page = requests.get(url).text\n",
    "    data = BeautifulSoup(page, 'html.parser')\n",
    "    df = pd.DataFrame(columns=table_attribs)\n",
    "    tables = data.find_all('tbody')\n",
    "    rows = tables[0].find_all('tr')\n",
    "    for row in rows:\n",
    "        col = row.find_all('td')\n",
    "        if (len(col)==0):\n",
    "            continue\n",
    "        if (col[1].find('a')==None):\n",
    "            continue\n",
    "        data_dict = {\"Name\": col[1].find_all('a')[1].contents[0],\n",
    "                     \"MC_USD_Billion\": float(col[2].contents[0].replace('\\n', ''))}\n",
    "        df1 = pd.DataFrame(data_dict, index=[0])\n",
    "        if df.empty:\n",
    "            df = df1.copy()\n",
    "        else:\n",
    "            df = pd.concat([df, df1], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def transform(df, csv_path):\n",
    "    ''' This function accesses the CSV file for exchange rate\n",
    "    information, and adds three columns to the data frame, each\n",
    "    containing the transformed version of Market Cap column to\n",
    "    respective currencies'''\n",
    "    exchange_rate = pd.read_csv(exchange_csv_path).set_index('Currency').to_dict()['Rate']\n",
    "    MC_USD_list = df['MC_USD_Billion'].tolist()\n",
    "    for key, val in exchange_rate.items():\n",
    "        df[f'MC_{key}_Billion'] = list(map(lambda x: np.round(x*val, 2), MC_USD_list))\n",
    "    return df\n",
    "\n",
    "def load_to_csv(df, output_path):\n",
    "    ''' This function saves the final data frame as a CSV file in\n",
    "    the provided path. Function returns nothing.'''\n",
    "    df.to_csv(output_path)\n",
    "\n",
    "def load_to_db(df, sql_connection, table_name):\n",
    "    ''' This function saves the final data frame to a database\n",
    "    table with the provided name. Function returns nothing.'''\n",
    "    df.to_sql(table_name, sql_connection, if_exists='replace', index=False)\n",
    "\n",
    "def run_query(query_statement, sql_connection):\n",
    "    ''' This function runs the query on the database table and\n",
    "    prints the output on the terminal. Function returns nothing. '''\n",
    "    print(query_statement)\n",
    "    query_output = pd.read_sql(query_statement, sql_connection)\n",
    "    print(query_output)\n",
    "\n",
    "''' Here, you define the required entities and call the relevant\n",
    "functions in the correct order to complete the project. Note that this\n",
    "portion is not inside any function.'''\n",
    "\n",
    "url = 'https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks'\n",
    "table_attribs = ['Name', 'MC_USD_Billion']\n",
    "exchange_csv_path = 'exchange_rate.csv'\n",
    "output_csv_path = 'Largest_banks_data.csv'\n",
    "table_name = 'Largest_banks'\n",
    "\n",
    "log_progress('Preliminaries complete. Initiating ETL process')\n",
    "\n",
    "df = extract(url, table_attribs)\n",
    "print(df)\n",
    "log_progress('Data extraction complete. Initiating Transformation process')\n",
    "\n",
    "df = transform(df, exchange_csv_path )\n",
    "print(df)\n",
    "log_progress('Data transformation complete. Initiating loading process')\n",
    "\n",
    "load_to_csv(df, output_csv_path)\n",
    "\n",
    "log_progress('Data saved to CSV file')\n",
    "\n",
    "sql_connection = sqlite3.connect('Banks.db')\n",
    "\n",
    "log_progress('SQL Connection initiated.')\n",
    "\n",
    "load_to_db(df, sql_connection, table_name)\n",
    "\n",
    "log_progress('Data loaded to Database as table. Running the query')\n",
    "\n",
    "query_statement_1 = f'SELECT * FROM {table_name}'\n",
    "run_query(query_statement_1, sql_connection)\n",
    "\n",
    "query_statement_2 = f'SELECT AVG(MC_GBP_Billion) FROM {table_name}'\n",
    "run_query(query_statement_2, sql_connection)\n",
    "\n",
    "query_statement_3 = f'SELECT Name from {table_name} LIMIT 5'\n",
    "run_query(query_statement_3, sql_connection)\n",
    "\n",
    "log_progress('Process Complete.')\n",
    "\n",
    "sql_connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
